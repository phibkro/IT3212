{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Explore the dataset by displaying the first few rows, summary statistics, and data types of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Area                 Item         Element  Year Unit   Value\n",
      "0  Afghanistan  Almonds, with shell  Area harvested  1975   ha     0.0\n",
      "1  Afghanistan  Almonds, with shell  Area harvested  1976   ha  5900.0\n",
      "2  Afghanistan  Almonds, with shell  Area harvested  1977   ha  6000.0\n",
      "3  Afghanistan  Almonds, with shell  Area harvested  1978   ha  6000.0\n",
      "4  Afghanistan  Almonds, with shell  Area harvested  1979   ha  6000.0\n",
      "Area        object\n",
      "Item        object\n",
      "Element     object\n",
      "Year         int64\n",
      "Unit        object\n",
      "Value      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import the smoking dataset\n",
    "df = pd.read_csv('../../data/food_bank/crop1.csv')\n",
    "# Display first few rows\n",
    "print(df.head())\n",
    "# Summary\n",
    "df.describe()\n",
    "# Data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Identify missing values, outliers, and unique values in categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " Area            0\n",
      "Item            0\n",
      "Element         0\n",
      "Year            0\n",
      "Unit            0\n",
      "Value      129500\n",
      "dtype: int64\n",
      "Unique Areas:245\n",
      "Unique Elements:3\n",
      "Unique Items:118\n",
      "Unique Units:3\n"
     ]
    }
   ],
   "source": [
    "# Identify missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Identify unique values in categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "unique_values = {col: df[col].unique() for col in categorical_columns}\n",
    "\n",
    "print(f\"Unique Areas:{len(unique_values.get('Area'))}\")\n",
    "print(f\"Unique Elements:{len(unique_values.get('Element'))}\")\n",
    "print(f\"Unique Items:{len(unique_values.get('Item'))}\")\n",
    "print(f\"Unique Units:{len(unique_values.get('Unit'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that there are lots of missing \"value\" values.\n",
    "These are often the same crop, thus we have no good data for that particular crop anyways.\n",
    "We might just drop the rows where the \"value\" is missing as it will not help improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1975</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1976</td>\n",
       "      <td>5900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1977</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1978</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1979</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Value\n",
       "0  1975     0.0\n",
       "1  1976  5900.0\n",
       "2  1977  6000.0\n",
       "3  1978  6000.0\n",
       "4  1979  6000.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out categorial columns, leaving only numerical\n",
    "df_numerical = df.select_dtypes(include=[np.number])\n",
    "df_numerical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Detect outliers using methods such as the IQR method or Z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outliers:\n",
      " Year          0\n",
      "Value    246096\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify outliers using the IQR method\n",
    "Q1 = df_numerical.quantile(0.25)\n",
    "Q3 = df_numerical.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((df_numerical < (Q1 - 1.5 * IQR)) | (df_numerical > (Q3 + 1.5 * IQR))).sum()\n",
    "print(\"\\nOutliers:\\n\", outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Decide whether to remove, cap, or transform the outliers. Justify your decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Apply label encoding or one-hot encoding to transform categorical data into \n",
    "numerical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns amount: 371\n"
     ]
    }
   ],
   "source": [
    "# Apply one-hot encoding to categorical columns\n",
    "categorial_df = pd.get_dummies(df, columns=['Area', 'Element', 'Item', 'Unit'])\n",
    "categorial_df.head()\n",
    "\n",
    "# Print columns amount\n",
    "print(f\"Columns amount: {len(categorial_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we use one-hot encoding over label encoding?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Apply feature scaling techniques such as normalization (Min-Max scaling) or \n",
    "standardization (Z-score normalization) to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MinMaxScaler' from 'sklearn' (/usr/local/lib/python3.11/site-packages/sklearn/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MinMaxScaler' from 'sklearn' (/usr/local/lib/python3.11/site-packages/sklearn/__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Explain why feature scaling is necessary and how it impacts the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Split the preprocessed dataset into training and testing sets. Typically, an 80-20 or 70-30 split is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Explain the importance of splitting the data and how it prevents overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
